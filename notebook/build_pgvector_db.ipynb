{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Disclaimer**\n",
    "This Jupyter notebook is derived from and builds upon the following notebook. Credit and appreciation are extended to the original author(s) for their foundational work, which has been adapted and expanded for the current purpose.\n",
    "https://github.com/build-on-aws/langchain-embeddings/blob/main/notebooks/03_build_pgvector_db.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Supercharging Vector Similarity Search with Amazon Aurora and pgvector\n",
    "In this Jupyter Notebook, you'll explore how to store vector embeddings in a vector database using [Amazon Aurora](https://aws.amazon.com/es/rds/aurora/) and the pgvector extension. This approach is particularly useful for applications that require efficient similarity searches on high-dimensional data, such as natural language processing, image recognition, and recommendation systems.\n",
    "\n",
    "[Amazon Aurora](https://aws.amazon.com/es/rds/aurora/) is a fully managed relational database service provided by Amazon Web Services (AWS). It is compatible with PostgreSQL and supports the [pgvector](https://github.com/pgvector/pgvector) extension, which introduces a 'vector' data type and specialized query operators for vector similarity searches. The pgvector extension utilizes the ivfflat indexing mechanism to expedite these searches, allowing you to store and index up to 16,000 dimensions, while optimizing search performance for up to 2,000 dimensions.\n",
    "\n",
    "For developers and data engineers with experience in relational databases and PostgreSQL, Amazon Aurora with pgvector offers a powerful and familiar solution for managing vector datastores, especially when dealing with structured datasets. Alternatively, Amazon Relational Database Service (RDS) for PostgreSQL is also a suitable option, particularly if you require specific PostgreSQL versions.\n",
    "\n",
    "Both Amazon Aurora and Amazon RDS for PostgreSQL offer horizontal scaling capabilities for read queries, with a maximum of 15 replicas. Additionally, Amazon Aurora PostgreSQL provides a Serverless v2 option, which automatically scales compute and memory resources based on your application's demand, simplifying operations and capacity planning."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "**0- Set up dependencies:**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q psycopg[binary] langchain_postgres langchain_community langchain_aws langchain_experimental datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "from langchain_community.document_loaders import DataFrameLoader\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_postgres import PGVector\n",
    "from langchain_aws import ChatBedrock, BedrockEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.callbacks import StdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set up database connection:** Ensure that you have an Amazon Aurora instance configured and running. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "session = boto3.session.Session()\n",
    "client = session.client(\n",
    "    service_name='secretsmanager',\n",
    ")\n",
    "\n",
    "response = client.get_secret_value(SecretId=\"phoenix-demo-db-credential\")\n",
    "secret = json.loads(response['SecretString'])\n",
    "print(secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import psycopg\n",
    "\n",
    "connection = f\"postgresql://{secret['username']}:{secret['password']}@{secret['host']}:{secret['port']}/{secret['dbname']}\"\n",
    "\n",
    "# Establish the connection to the database\n",
    "conn = psycopg.connect(\n",
    "    conninfo = connection\n",
    ")\n",
    "# Create a cursor to run queries\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load HuggingFace Dataset to PG Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bedrock_client = boto3.client(\"bedrock-runtime\", region_name=\"us-east-1\") \n",
    "bedrock_embeddings = BedrockEmbeddings(model_id=\"amazon.titan-embed-text-v2:0\", client=bedrock_client)\n",
    "llm = ChatBedrock(model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\", client=bedrock_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load aws_whitepapers dataset from huggingface\n",
    "ds = load_dataset(\"si3mshady/aws_whitepapers\")\n",
    "\n",
    "# Convert dataset to dataframe\n",
    "df = pd.DataFrame(ds[\"train\"])\n",
    "\n",
    "# Check data shape\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_and_split_semantic(embeddings):\n",
    "    loader = DataFrameLoader(df, page_content_column=\"Content\")\n",
    "    docs = loader.load_and_split()\n",
    "    print(f\"docs:{len(docs)}\")\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# function to create vector store\n",
    "def create_vectorstore(embeddings, collection_name, conn):\n",
    "    vectorstore = PGVector(\n",
    "        embeddings=embeddings,\n",
    "        collection_name=collection_name,\n",
    "        connection=conn,\n",
    "        use_jsonb=True,\n",
    "    )\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "docs = load_and_split_semantic(bedrock_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "collection_name_text = \"aws_whitepapers\"\n",
    "vectorstore = create_vectorstore(bedrock_embeddings, collection_name_text, connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add documents to the vectorstore\n",
    "# this will take roughly 10-15 minutes.\n",
    "vectorstore.add_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify successful loading of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vectorstore.similarity_search(\"what are the pillars in AWS well architected framework?\", k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vectorstore.similarity_search_with_relevance_scores(\"what is the durability of s3?\", k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve information using Amazon Bedrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "You are an AI assistant tasked with answering questions based on provided context. Your goal is to provide accurate and relevant answers using only the information given.\n",
    "\n",
    "Here is the context you should use to answer the question:\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Now, here is the question you need to answer:\n",
    "\n",
    "<question>\n",
    "{query}\n",
    "</question>\n",
    "\n",
    "Instructions:\n",
    "1. Carefully read and analyze the provided context.\n",
    "2. Identify key information in the context that is relevant to the question.\n",
    "3. Formulate an answer to the question using only the information from the given context.\n",
    "4. If the context does not contain enough information to fully answer the question, state this clearly in your response.\n",
    "5. Do not use any external knowledge or information not present in the provided context.\n",
    "6. Keep your answer concise and to the point, while ensuring it fully addresses the question.\n",
    "\n",
    "Format your response as follows:\n",
    "1. Begin with a brief answer to the question.\n",
    "2. Follow with a more detailed explanation, if necessary.\n",
    "3. If you're quoting directly from the context, use quotation marks and indicate the quote's location in the context.\n",
    "\n",
    "Remember, it's important to rely solely on the given context and not to introduce any external information or assumptions in your answer.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"what is the durability of s3?\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "\n",
    "def parse_docs(docs):\n",
    "    return {\n",
    "        'query': query,\n",
    "        'context': docs\n",
    "    }\n",
    "\n",
    "\n",
    "llm = ChatBedrock(\n",
    "    model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "    region_name=\"us-east-1\",\n",
    ")\n",
    "\n",
    "chain = vectorstore.as_retriever() | parse_docs | prompt | llm\n",
    "\n",
    "print(chain.invoke(query))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn more: \n",
    "- [Leverage pgvector and Amazon Aurora PostgreSQL for Natural Language Processing, Chatbots and Sentiment Analysis](https://aws.amazon.com/es/blogs/database/leverage-pgvector-and-amazon-aurora-postgresql-for-natural-language-processing-chatbots-and-sentiment-analysis/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete vectorDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore.drop_tables()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
