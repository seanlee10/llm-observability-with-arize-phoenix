{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Disclaimer**\n",
    "This Jupyter notebook is derived from and builds upon the following notebook. Credit and appreciation are extended to the original author(s) for their foundational work, which has been adapted and expanded for the current purpose.\n",
    "https://github.com/build-on-aws/langchain-embeddings/blob/main/notebooks/03_build_pgvector_db.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Supercharging Vector Similarity Search with Amazon Aurora and pgvector\n",
    "In this Jupyter Notebook, you'll explore how to store vector embeddings in a vector database using [Amazon Aurora](https://aws.amazon.com/es/rds/aurora/) and the pgvector extension. This approach is particularly useful for applications that require efficient similarity searches on high-dimensional data, such as natural language processing, image recognition, and recommendation systems.\n",
    "\n",
    "[Amazon Aurora](https://aws.amazon.com/es/rds/aurora/) is a fully managed relational database service provided by Amazon Web Services (AWS). It is compatible with PostgreSQL and supports the [pgvector](https://github.com/pgvector/pgvector) extension, which introduces a 'vector' data type and specialized query operators for vector similarity searches. The pgvector extension utilizes the ivfflat indexing mechanism to expedite these searches, allowing you to store and index up to 16,000 dimensions, while optimizing search performance for up to 2,000 dimensions.\n",
    "\n",
    "For developers and data engineers with experience in relational databases and PostgreSQL, Amazon Aurora with pgvector offers a powerful and familiar solution for managing vector datastores, especially when dealing with structured datasets. Alternatively, Amazon Relational Database Service (RDS) for PostgreSQL is also a suitable option, particularly if you require specific PostgreSQL versions.\n",
    "\n",
    "Both Amazon Aurora and Amazon RDS for PostgreSQL offer horizontal scaling capabilities for read queries, with a maximum of 15 replicas. Additionally, Amazon Aurora PostgreSQL provides a Serverless v2 option, which automatically scales compute and memory resources based on your application's demand, simplifying operations and capacity planning."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "**0- Set up dependencies:**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "astropy 6.1.0 requires numpy>=1.23, but you have numpy 1.22.4 which is incompatible.\n",
      "scikit-image 0.23.2 requires numpy>=1.23, but you have numpy 1.22.4 which is incompatible.\n",
      "sparkmagic 0.21.0 requires pandas<2.0.0,>=0.17.1, but you have pandas 2.2.2 which is incompatible.\n",
      "sphinx 7.3.7 requires docutils<0.22,>=0.18.1, but you have docutils 0.16 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q psycopg[binary] langchain_postgres langchain_community langchain_aws langchain_experimental datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "from langchain_community.document_loaders import DataFrameLoader\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_postgres import PGVector\n",
    "from langchain_aws import ChatBedrock, BedrockEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.callbacks import StdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1- Set up database connection:** Ensure that you have an Amazon Aurora instance configured and running. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dbClusterIdentifier': 'phoenix-demo-db', 'password': 'Y535lJ462DlzvIu7SNtVTuWFbjXDcDs9', 'dbname': 'postgres', 'engine': 'postgres', 'port': 5432, 'host': 'phoenix-demo-db.cluster-c56c0icccghq.us-east-1.rds.amazonaws.com', 'username': 'postgres'}\n"
     ]
    }
   ],
   "source": [
    "session = boto3.session.Session()\n",
    "client = session.client(\n",
    "    service_name='secretsmanager',\n",
    ")\n",
    "\n",
    "response = client.get_secret_value(SecretId=\"phoenix-demo-db-credential\")\n",
    "secret = json.loads(response['SecretString'])\n",
    "print(secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import psycopg\n",
    "\n",
    "connection = f\"postgresql://{secret['username']}:{secret['password']}@{secret['host']}:{secret['port']}/{secret['dbname']}\"\n",
    "\n",
    "# Establish the connection to the database\n",
    "conn = psycopg.connect(\n",
    "    conninfo = connection\n",
    ")\n",
    "# Create a cursor to run queries\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2- Enable the [pgvector extension](https://docs.aws.amazon.com/AmazonRDS/latest/AuroraPostgreSQLReleaseNotes/AuroraPostgreSQL.Extensions.html?sc_channel=el&sc_campaign=genai&sc_geo=mult&sc_country=mult&sc_outcome=acq&sc_content=vector-embeddings-and-rag-demystified-2):** Once connected to your Aurora instance, enable the pgvector extension by running the following SQL command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cur.execute(\"CREATE EXTENSION vector;\")\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3- Create a table to store embeddings:** Define a table schema to store your vector embeddings and any associated metadata. \n",
    "\n",
    "This table includes columns for a unique identifier (id), the original text (text), and the vector embedding (embedding) with a dimensionality of 1536."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "table_name = \"embeddings\"\n",
    "query = f\"\"\"CREATE TABLE {table_name} (\n",
    "    id SERIAL PRIMARY KEY,\n",
    "    text TEXT,\n",
    "    embedding VECTOR(1536)\n",
    ");\"\"\"\n",
    "cur.execute(query)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load HuggingFace Dataset to PG Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bedrock_client = boto3.client(\"bedrock-runtime\", region_name=\"us-east-1\") \n",
    "bedrock_embeddings = BedrockEmbeddings(model_id=\"amazon.titan-embed-text-v2:0\", client=bedrock_client)\n",
    "llm = ChatBedrock(model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\", client=bedrock_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f62712f793b4a10abe0a0afabcea46f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/10.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fc51d01a3a24c5e9f7e73b5f9265f15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/209 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Content</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Amazon_Aurora_Migration_Handbook</td>\n",
       "      <td>This paper has been archived For the latest Am...</td>\n",
       "      <td>General</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A_Practical_Guide_to_Cloud_Migration_Migrating...</td>\n",
       "      <td>Archived A Practical Gui de to Cl oud Migratio...</td>\n",
       "      <td>General</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Amazon_Aurora_MySQL_Database_Administrators_Ha...</td>\n",
       "      <td>This version has been archived For the latest ...</td>\n",
       "      <td>General</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A_Platform_for_Computing_at_the_Mobile_Edge_Jo...</td>\n",
       "      <td>ArchivedA Platform for Computing at the Mobile...</td>\n",
       "      <td>General</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Amazon_Elastic_File_System_Choosing_Between_Di...</td>\n",
       "      <td>This paper has been archived For the latest te...</td>\n",
       "      <td>General</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0                   Amazon_Aurora_Migration_Handbook   \n",
       "1  A_Practical_Guide_to_Cloud_Migration_Migrating...   \n",
       "2  Amazon_Aurora_MySQL_Database_Administrators_Ha...   \n",
       "3  A_Platform_for_Computing_at_the_Mobile_Edge_Jo...   \n",
       "4  Amazon_Elastic_File_System_Choosing_Between_Di...   \n",
       "\n",
       "                                             Content Category  \n",
       "0  This paper has been archived For the latest Am...  General  \n",
       "1  Archived A Practical Gui de to Cl oud Migratio...  General  \n",
       "2  This version has been archived For the latest ...  General  \n",
       "3  ArchivedA Platform for Computing at the Mobile...  General  \n",
       "4  This paper has been archived For the latest te...  General  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load aws_whitepapers dataset from huggingface\n",
    "ds = load_dataset(\"si3mshady/aws_whitepapers\")\n",
    "\n",
    "# Convert dataset to dataframe\n",
    "df = pd.DataFrame(ds[\"train\"])\n",
    "\n",
    "# Check data shape\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_and_split_semantic(embeddings):\n",
    "    loader = DataFrameLoader(df, page_content_column=\"Content\")\n",
    "    docs = loader.load_and_split()\n",
    "    print(f\"docs:{len(docs)}\")\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# function to create vector store\n",
    "def create_vectorstore(embeddings, collection_name, conn):\n",
    "    vectorstore = PGVector(\n",
    "        embeddings=embeddings,\n",
    "        collection_name=collection_name,\n",
    "        connection=conn,\n",
    "        use_jsonb=True,\n",
    "    )\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docs:2918\n"
     ]
    }
   ],
   "source": [
    "docs = load_and_split_semantic(bedrock_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "collection_name_text = \"aws_whitepapers\"\n",
    "vectorstore = create_vectorstore(bedrock_embeddings, collection_name_text, connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add documents to the vectorstore\n",
    "# this will take roughly 10-15 minutes.\n",
    "vectorstore.add_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify successful loading of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Title': 'AWS_WellArchitected_Framework', 'Category': 'General'}, page_content='ArchivedAWS WellArchitected Framework July 2020 This whitepaper describes the AWS WellArchitected Framework It provides guidance to help cus tomers apply best practices in the design delivery and maintenance of AWS environments We address general design principles as well as specific best practices and guidance in ﬁve conceptual areas that we define as the pillars of the WellArchitected FrameworkThis paper has been archived The latest version is available at: https://docsawsamazoncom/wellarchitected/latest/framework/welcomehtmlArchivedAWS WellArchitected Framework Notices Customers are responsible for making their own independent assessment of the in formation in this document This document: (a) is for informational purposes only (b) represents current AWS product offerings and practices which are subject to change without notice and (c) does not create any commitments or assurances from AWS and its affiliates suppliers or licensors AWS products or services are provided “as is” without warranties representations or conditions of any kind whether express or implied The responsibilities and liabilities of AWS to its customers are controlled by AWS agreements and this document is not part of nor does it modify any agreement between AWS and its customers Copyright © 2020 Amazon Web Services Inc or its affiliatesArchivedAWS WellArchitected Framework Introduction  1 Definitions  2 On Architecture  3 General Design Principles  5 The Five Pillars of the Framework  6 Operational Excellence  6 Security  15 Reliability  22 Performance Efficiency  28 Cost Optimization  36 The Review Process  43 Conclusion  45 Contributors  46 Further Reading  47 Document Revisions  48 Appendix: Questions and Best Practices  49 Operational Excellence  49 Security  60 Reliability  69 Performance Efficiency  80 Cost Optimization  88 iiiArchivedAWS WellArchitected Framework Introduction The AWS WellArchitected Framework helps you understand the pros and cons of de cisions you make while building systems on AWS By using the Framework you will learn architectural best practices for designing and operating reliable secure effi cient and costeffective systems in the cloud It provides a way for you to consistently measure your architectures against best practices and identify areas for improvement The process for reviewing an architecture is a constructive conversation about archi tectural decisions and is not an audit mechanism We believe that having wellarchi tected systems greatly increases the likelihood of business success AWS Solutions Architects have years of experience architecting solutions across a wide variety of business verticals and use cases We have helped design and review thou sands of customers’ architectures on AWS From this experience we have identified best practices and core strategies for architecting systems in the cloud The AWS WellArchitected Framework documents a set of foundational questions that allow you to understand if a specific architecture aligns well with cloud best practices The framework provides a consistent approach to evaluating systems against the qualities you expect from modern cloudbased systems and the remedi ation that would be required to achieve those qualities As AWS continues to evolve and we continue to learn more from working with our customers we will continue to refine the definition of wellarchitected This framework is intended for those in technology roles such as chief technology of ficers (CTOs) architects developers and operations team members It describes AWS best practices and strategies to use when designing and operating a cloud workload and provides links to further implementation details and architectural patterns For more information see the AWS WellArchitected homepage AWS also provides a service for reviewing your workloads at no charge The AWS WellArchitected Tool (AWS WA Tool) is a service in the cloud that provides a consis tent process for you to review and measure your architecture using the'),\n",
       " Document(metadata={'Title': 'AWS_WellArchitected_Framework', 'Category': 'General'}, page_content='your workloads at no charge The AWS WellArchitected Tool (AWS WA Tool) is a service in the cloud that provides a consis tent process for you to review and measure your architecture using the AWS WellAr chitected Framework The AWS WA Tool provides recommendations for making your workloads more reliable secure efficient and costeffective To help you apply best practices we have created AWS WellArchitected Labs which provides you with a repository of code and documentation to give you handson ex perience implementing best practices We also have teamed up with select AWS Part ner Network (APN) Partners who are members of the AWS WellArchitected Partner program  These APN Partners have deep AWS knowledge and can help you review and improve your workloads 1ArchivedAWS WellArchitected Framework Definitions Every day experts at AWS assist customers in architecting systems to take advantage of best practices in the cloud We work with you on making architectural tradeoﬀs as your designs evolve As you deploy these systems into live environments we learn how well these systems perform and the consequences of those tradeoﬀs Based on what we have learned we have created the AWS WellArchitected Frame work which provides a consistent set of best practices for customers and partners to evaluate architectures and provides a set of questions you can use to evaluate how well an architecture is aligned to AWS best practices The AWS WellArchitected Framework is based on ﬁve pillars — operational excel lence security reliability performance efficiency and cost optimization Table 1 The pillars of the AWS WellArchitected Framework Name Description Operational Excellence The ability to support development and run workloads effectively gain insight into their operations and to continuously improve supporting processes and proce dures to deliver business value Security The security pillar encompasses the ability to protect data systems and assets to take advantage of cloud technologies to improve your security Reliability The reliability pillar encompasses the ability of a work load to perform its intended function correctly and con sistently when it’s expected to This includes the ability to operate and test the workload through its total life cycle This paper provides indepth best practice guid ance for implementing reliable workloads on AWS Performance Efficiency The ability to use computing resources efficiently to meet system requirements and to maintain that effi ciency as demand changes and technologies evolve Cost Optimization The ability to run systems to deliver business value at the lowest price point In the AWS WellArchitected Framework we use these terms: • A component is the code configuration and AWS Resources that together deliver against a requirement A component is often the unit of technical ownership and is decoupled from other components 2ArchivedAWS WellArchitected Framework • The term workload is used to identify a set of components that together deliver business value A workload is usually the level of detail that business and technolo gy leaders communicate about • We think about architecture as being how components work together in a work load How components communicate and interact is often the focus of architecture diagrams •Milestones mark key changes in your architecture as it evolves throughout the product lifecycle (design testing go live and in production) • Within an organization the technology portfolio is the collection of workloads that are required for the business to operate When architecting workloads you make tradeoﬀs between pillars based on your business context These business decisions can drive your engineering priorities You might optimize to reduce cost at the expense of reliability in development environ ments or for missioncritical solutions you might optimize reliability with increased costs In ecommerce solutions performance can affect revenue and customer propen sity to buy Security and operational'),\n",
       " Document(metadata={'Title': 'AWS_WellArchitected_Framework', 'Category': 'General'}, page_content='one or two meet ings where you can gain clarity or dive deep on areas of ambiguity or perceived risk Here are some suggested items to facilitate your meetings: • A meeting room with whiteboards 1Many decisions are reversible twoway doors Those decisions can use a light weight process Oneway doors are hard or impossible to reverse and require more inspection before making them 43ArchivedAWS WellArchitected Framework • Print outs of any diagrams or design notes • Action list of questions that require outofband research to answer (for example “did we enable encryption or not?” ) After you have done a review you should have a list of issues that you can prioritize based on your business context You will also want to take into account the impact of those issues on the daytoday work of your team If you address these issues early you could free up time to work on creating business value rather than solving recur ring problems As you address issues you can update your review to see how the ar chitecture is improving While the value of a review is clear after you have done one you may ﬁnd that a new team might be resistant at ﬁrst Here are some objections that can be handled through educating the team on the benefits of a review: • “We are too busy!” (Often said when the team is getting ready for a big launch) • If you are getting ready for a big launch you will want it to go smoothly The re view will allow you to understand any problems you might have missed • We recommend that you carry out reviews early in the product lifecycle to uncov er risks and develop a mitigation plan aligned with the feature delivery roadmap • “We don’t have time to do anything with the results!” (Often said when there is an immovable event such as the Super Bowl that they are targeting) • These events can’t be moved Do you really want to go into it without knowing the risks in your architecture? Even if you don’t address all of these issues you can still have playbooks for handling them if they materialize • “We don’t want others to know the secrets of our solution implementation!” • If you point the team at the questions in the WellArchitected Framework they will see that none of the questions reveal any commercial or technical propriety information As you carry out multiple reviews with teams in your organization you might identify thematic issues For example you might see that a group of teams has clusters of is sues in a particular pillar or topic You will want to look at all your reviews in a holis tic manner and identify any mechanisms training or principal engineering talks that could help address those thematic issues 44ArchivedAWS WellArchitected Framework Conclusion The AWS WellArchitected Framework provides architectural best practices across the ﬁve pillars for designing and operating reliable secure efficient and costeffective systems in the cloud The Framework provides a set of questions that allows you to review an existing or proposed architecture It also provides a set of AWS best prac tices for each pillar Using the Framework in your architecture will help you produce stable and efficient systems which allow you to focus on your functional require ments 45ArchivedAWS WellArchitected Framework Contributors The following individuals and organizations contributed to this document: • Rodney Lester: Senior Manager WellArchitected Amazon Web Services • Brian Carlson: Operations Lead WellArchitected Amazon Web Services • Ben Potter: Security Lead WellArchitected Amazon Web Services • Eric Pullen: Performance Lead WellArchitected Amazon Web Services • Seth Eliot: Reliability Lead WellArchitected Amazon Web Services • Nathan Besh: Cost Lead WellArchitected Amazon Web Services • Jon Steele: Sr Technical Account Manager Amazon Web Services • Ryan King: Technical Program Manager Amazon Web Services • Erin Rifkin: Senior Product Manager Amazon Web Services • Max Ramsay: Principal Security Solutions Architect Amazon Web Services • Scott Paddock:'),\n",
       " Document(metadata={'Title': 'AWS_WellArchitected_Framework__Security_Pillar', 'Category': 'General'}, page_content='ArchivedSecurity Pillar AWS Well Architected Framework July 2020 This paper has been archived The latest version is now available at: https://docsawsamazoncom/wellarchitected/latest/securitypillar/welcomehtmlArchivedNotices Customers are responsible for making their own independent assessment of the information in this document This document: (a) is for informational purposes only (b) represents current AWS product offerings and practices which are subject to change withou t notice and (c) does not create any commitments or assurances from AWS and its affiliates suppliers or licensors AWS products or services are provided “as is” without warranties representations or conditions of any kind whether express or implied The responsibilities and liabilities of AWS to its customers are controlled by AWS agreements and this document is not part of nor does it modify any agreement between AWS and its customers © 20 20 Amazon Web Services Inc or its affiliates All rights reserved ArchivedContents Introduction     1 Security     2 Design Principles     2 Definition     3 Operating Your Workload Securely    3 AWS Account Management and Separation    5 Identity and Access Management    7 Identity Management     7 Permissions Management    11 Detection     15 Configure     15 Investigate     18 Infrastructure Protect ion     19 Protecting Networks     20 Protecting Compute     23 Data Protection     27 Data Classification     27 Protecting Dat a at Rest     29 Protecting Data in Transit    32 Incident Response     34 Design Goals of Cloud Response    34 Educate     35 Prepare     36 Simulate     38 Iterate     39 Conclusion     40 ArchivedContributors     40 Further Reading     41 Document Revisions     41 ArchivedAbstract The focus of this paper is the security pillar of the WellArchitected Framework  It provides guidance to help you apply best practices current recommendations in the design delivery and maintenance of secure AWS workloads  ArchivedAmazon Web Services Security Pillar 1 Introduction The AWS Well Architected Framework helps you understand trade offs for decisions you make while building workloads on AWS By using the Framework you will learn current architectural best practices for designing and operating reliable secure efficient and cost effective workloads in the cloud It provides a way fo r you to consistently measure your workload against best practices and identify areas for improvement We believe that having well architected workload s greatly increases the likelihood of business success The framework is based on five pillars: • Operation al Excellence • Security • Reliability • Performance Efficiency • Cost Optimization This paper focuses on the security pillar This will help you meet your business and regulatory requirements by following current AWS recommendations It’s intended for those in technology roles such as chief technology officers (CTOs) chief information security officers (CSOs/CISOs) architects developers and operations team members After reading this paper you will understand AWS current recommendations and strategies to use when designing cloud architectures with security in mind This paper doesn ’t provide implementation details or architectural patterns but does include references to appropriate resources for this information By adopting the practices in this paper you can build architectures that protect your data and systems control access and respond automatically to security events ArchivedAmazon Web Services Security P illar 2 Security The security pillar describes how to take advantage of cloud technologies to protect data systems and assets in a way that can improve your security posture This paper provides in depth best practice guidance for architecting secure workloads on AWS Design Principles In the cloud there are a number of principles that can help you strengthen your workload security: • Implement a strong identity foundation: Implement the principle of least privilege and enforce'),\n",
       " Document(metadata={'Title': 'AWS_WellArchitected_Framework', 'Category': 'General'}, page_content=\"could ad dress by mechanisms training or lunchtime talks where your principal engineers can share their thinking on specific areas with multiple teams 3Working backward is a fundamental part of our innovation process We start with the customer and what they want and let that define and guide our efforts 4ArchivedAWS WellArchitected Framework General Design Principles The WellArchitected Framework identifies a set of general design principles to facili tate good design in the cloud: •Stop guessing your capacity needs : If you make a poor capacity decision when de ploying a workload you might end up sitting on expensive idle resources or deal ing with the performance implications of limited capacity With cloud computing these problems can go away You can use as much or as little capacity as you need and scale up and down automatically •Test systems at production scale : In the cloud you can create a productionscale test environment on demand complete your testing and then decommission the resources Because you only pay for the test environment when it's running you can simulate your live environment for a fraction of the cost of testing on premises •Automate to make architectural experimentation easier: Automation allows you to create and replicate your workloads at low cost and avoid the expense of manu al effort You can track changes to your automation audit the impact and revert to previous parameters when necessary •Allow for evolutionary architectures: Allow for evolutionary architectures In a tra ditional environment architectural decisions are often implemented as static one time events with a few major versions of a system during its lifetime As a business and its context continue to evolve these initial decisions might hinder the system's ability to deliver changing business requirements In the cloud the capability to au tomate and test on demand lowers the risk of impact from design changes This al lows systems to evolve over time so that businesses can take advantage of innova tions as a standard practice •Drive architectures using data : In the cloud you can collect data on how your ar chitectural choices affect the behavior of your workload This lets you make fact based decisions on how to improve your workload Your cloud infrastructure is code so you can use that data to inform your architecture choices and improve ments over time •Improve through game days : Test how your architecture and processes perform by regularly scheduling game days to simulate events in production This will help you understand where improvements can be made and can help develop organizational experience in dealing with events 5ArchivedAWS WellArchitected Framework The Five Pillars of the Framework Creating a software system is a lot like constructing a building If the foundation is not solid structural problems can undermine the integrity and function of the build ing When architecting technology solutions if you neglect the ﬁve pillars of opera tional excellence security reliability performance efficiency and cost optimization it can become challenging to build a system that delivers on your expectations and re quirements Incorporating these pillars into your architecture will help you produce stable and efficient systems This will allow you to focus on the other aspects of de sign such as functional requirements Operational Excellence The Operational Excellence pillar includes the ability to support development and run workloads effectively gain insight into their operations and to continuously improve supporting processes and procedures to deliver business value The operational excellence pillar provides an overview of design principles best prac tices and questions You can ﬁnd prescriptive guidance on implementation in the Op erational Excellence Pillar whitepaper Design Principles There are ﬁve design principles for operational excellence in the cloud: •Perform operations as code : In the cloud you can apply the same engineering dis\")]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore.similarity_search(\"what are the pillars in AWS well architected framework?\", k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(metadata={'Title': 'AWS_Storage_Services_Overview', 'Category': 'General'}, page_content='to serve as the primary data storage for ArchivedAmazon Web Services – AWS Storage Services Overview Page 5 mission critical data In fact Amazon S3 is designed for 99999999999 percent (11 nines) durability per o bject and 9999 percent availability over a one year period Additionally you have a choice of enabling cross region replication on each Amazon S3 bucket Once enabled cross region replication automatically copies objects across buckets in different AWS Regions asynchronously providing 11 nines of durability and 4 nines of availability on both the source and destination Amazon S3 objects Scalability and Elasticity Amazon S3 has been designed to offer a very high level of automatic scalability and elasti city Unlike a typical file system that encounters issues when storing a large number of files in a directory Amazon S3 supports a virtually unlimited number of files in any bucket Also unlike a disk drive that has a limit on the total amount of data th at can be stored before you must partition the data across drives and/or servers an Amazon S3 bucket can store a virtually unlimited number of bytes You can store any number of objects (files) in a single bucket and Amazon S3 will automatically manage s caling and distributing redundant copies of your information to other servers in other locations in the same Region all using Amazon’s high performance infrastructure Security Amazon S3 is highly secure  It provides multiple mechanisms for fine grained control of access to Amazon S3 resources and it supports encryption You can manage access to Amazon S3 by granting other AWS accounts and users permission to perform the resource operations by writing an access policy 4 You can protect Amazon S3 data at rest by using serve rside encryption 5 in which you request Amazon S3 to encrypt your object before it’s written to disks in data centers and decrypt it when you download the object or by using client side encryption 6 in which you encrypt your data on the client side and upload the encrypted data to Amazon S3 You can protect the data in transit by using Secure Sockets Layer (SSL) or client side encryption ArchivedAmazon Web Services – AWS Storage Services Overview Page 6 You can use versioning to preserve retrieve and restore every version of every object stored in your Amazon S3 bucket With versioning you can easily recover from both unintended user actions and application failures Additionally you can add an optional layer of security by enabling Multi Factor Authentication (MFA) Delete for a bucket 7 With this option enabled for a bucket two forms of authentication are re quired to change the versioning state of the bucket or to permanently delete an object version: valid AWS account credentials plus a six  digit code (a single use time based password) from a physical or virtual token device To track requests for access t o your bucket you can enable access logging 8 Each access log record provides details about a single access request such as the requester bucket name request time request action response status and error code if any Access log information can be useful in security and access audits It can al so help you learn about your customer base and understand your Amazon S3 bill Interfaces Amazon S3 provides standards based REST web service application program interfaces (APIs) for both management and data operations These APIs allow Amazon S3 objects to be stored in uniquely named buckets (top level folders) Each object must have a unique object key (file name) that serves as an identifier for the object within that bucket Although Amazon S3 is a web based object store with a flat naming structure ra ther than a traditional file system you can easily emulate a file system hierarchy (folder1/folder2/file) in Amazon S3 by creating object key names that correspond to the full path name of each file Most developers building applications on Amazon S3 use a higher level toolkit or software development kit (SDK) that wraps'),\n",
       "  0.26434765841648966),\n",
       " (Document(metadata={'Title': 'Overview_of_AWS_Security__Storage_Services', 'Category': 'General'}, page_content=\"Condition) or based on the requester's client application (String Conditions) To identify these conditions you use policy keys For more information about action specific policy keys available within Amazon S3 refer to the Amazon Simple Storage Service Developer Guide  Amazon S3 also gives developers the option to use query string authentication which allows them to share Amazon S3 objects through URLs that are valid for a predefined period of time Query string authentication is useful for giving HTTP or browser access to resources that would normally require authentication The signature in the query string secures the request Data Transfer For maximum security you can securely upload/download data to Amazon S3 via the SSL encrypted endpoints The encrypted endpoints are accessible from both the Internet and from within Amazon EC2 so that data is transferred securely both within AWS and to and from sources outside of AWS Data Storage Amazon S3 provides multiple options for protecting data at rest For customers who prefer to manage their own encryption they can use a client encryption library like the Amazon S3 Encryption Client to encrypt data before uploading to Amazon S3 Alternatively you can use Amazon S3 Server Side Encryption (SSE) if you prefer to have Amazon S3 manage the encryption process for you Data is encrypted with a key generated by AWS or with a key you supply depending on your requirements With Amazon S3 SSE you can encrypt data on upload simply by adding an additional request header when writing the object Decryption happens automatically when data is retrieved Note that metadata which you can include with your object is not encrypted Therefore AWS recommends that customers not place sensitive information in Amazon S3 metadata Amazon S3 SSE uses one of the strongest block ciphers available – 256bit Advanced Encr yption Standard (AES 256) With Amazon S3 SSE every protected object is encrypted with a unique encryption key This object key itself is then encrypted with a regularly rotated master key Amazon S3 SSE provides additional security by storing the encrypted data and encryption keys in different hosts Amazon S3 SSE also makes it possible for you to enforce encryption requirements For example you can create and apply bucket policies that require that only encrypted data can be uploaded to your buckets For long term storage you can automatically archive the contents of your Amazon S3 buckets to AWS’ archival service called Amazon Glacier You can have data transferred at specific intervals to Glacier by creating lifecycle rules in Amazon S3 that describe which objects you want to be archived to Glacier and when As part of your data management strategy you can also specify how long Amazon S3 should wait after the objects are put into Amazon S3 to delete them When an object is deleted from Amazon S3 removal of the mapping from the public name Archived Page 5 of 9 to the object starts immediately and is generally processed across the distributed system within several seconds Once the mapping is removed there is no remote access to the deleted object The underlying storage area is then reclaimed for use by the system Data Durability and Reliability Amazon S3 is designed to provide 99999999999% durability and 9999% availability of objects over a given year Objects are redundantly stored on multiple devices across multi ple facilities in an Amazon S3 region To help provide durability Amazon S3 PUT and COPY operations synchronously store customer data across multiple facilities before returning SUCCESS Once stored Amazon S3 helps maintain the durability of the objects by quickly detecting and repairing any lost redundancy Amazon S3 also regularly verifies the integrity of data stored using checksums If corruption is detected it is repaired using redundant data In addition Amazon S3 calculates checksums on all netwo rk traffic to detect corruption of data packets when storing or retrieving data Amazon S3 provides\"),\n",
       "  0.1909997988939045),\n",
       " (Document(metadata={'Title': 'AWS_Answers_to_Key_Compliance_Questions', 'Category': 'General'}, page_content='of DDoS attacks Data portability Can the data stored with a service provider be exported by customer request? AWS allows customers to move data as needed on and off AWS storage AWS Import/Export service for S3 accelerates moving large amounts of data into and out of AWS using portable storage devices for transport Service provider business continuity Does the service provider operate a business continuity progr am? AWS does operate a business continuity program Detailed information is provided in the AWS Security Whitepaper Customer business continuity Does the service provider allow customers to implement a business continuity plan? AWS provides customers with the capability to implement a robust continuity plan including the utilization of frequent server instance back ups data redundancy replication and multi  region/availability zone deployment architectures Data durability Does the service specify data durability? Amazon S3 provides a highly durable storage infrastructure Objects are redundantly stored on multiple devices across multiple facilities in an Amazon S3 Region Once stored Amazon S3 maintains the durability of objects by quickly detectin g and repairing any lost redundancy Amazon S3 also regularly verifies the integrity of data stored using checksums If corruption is detected it is repaired using redundant data Data stored in S3 is designed to provide 99999999999% durability and 9999% availability of objects over a given year Backups Does the service provide backups to tapes? AWS allows customers to perform their own backups to tapes using their own tape backup service provider However a tape backup is not a service provided by AWS Amazon S3 service is designed to drive the likelihood of data loss to near zero percent and the durability equivalent of multi  site copies of data objects is achieved through data ArchivedAmazon Web Services – AWS Answers to Key Compliance Questions Page 7 Category Cloud Computing Question AWS Information storage redundancy For information on data durability and redundancy please refer to the AWS web site Price increases Will the service provider raise prices unexpectedly? AWS has a history of frequently reducing prices as the cost to provide these services reduces over time AWS has reduced prices consistently over the past several years Sustainability Does the service provider company have long term sustainability potential? AWS is a leading cloud provider and is a longterm business strategy of Amazoncom AWS has very high long term sustainability potential ArchivedAmazon Web Services – AWS Answers to Key Compliance Questions Page 8 Further Reading For additional information  see the following sources: • AWS Risk and Compliance Overview • AWS Certifications Program s Reports and Third Party Attestations • CSA Consensus Assessments Initiative Questionnaire Document Revisions Date Description January 2017 Migrated to new template January 2016 First publication'),\n",
       "  0.18903329968452587),\n",
       " (Document(metadata={'Title': 'AWS_Overview_of_Security_Processes', 'Category': 'General'}, page_content=\"multiple facilities in an Amazon S3 region To help provide durability Amazon S3 PUT and COPY operations synchronously store customer data across multiple facilities before returning SUCCESS Once stored Amazon S3 helps maintain the durability of ArchivedAmazon Web Services Amazon Web Services: Overview of Security Processes Page 46 the objects by quickly detecting and repairing any lost redundancy Amazon S3 also regularly verifies the integrity of data stored using checksums If corruption is detected it is repaired u sing redundant data In addition Amazon S3 calculates checksums on all network traffic to detect corruption of data packets when storing or retrieving data Amazon S3 provides further protection via Versioning You can use Versioning to preserve retrieve  and restore every version of every object stored in an Amazon S3 bucket With Versioning you can easily recover from both unintended user actions and application failures By default requests will retrieve the most recently written version Older versi ons of an object can be retrieved by specifying a version in the request You can further protect versions using Amazon S3 Versioning's MFA Delete feature Once enabled for an Amazon S3 bucket each version deletion request must include the six  digit code and serial number from your multi factor authentication device Access Logs An Amazon S3 bucket can be configured to log access to the bucket and objects within it The access log contains details about each access request including request type the requested resource the requestor’s IP and the time and date of the request When logging is enabled for a bucket log records are periodically aggregated into log files and delivered to the specified Amazon S3 bucket Cross Origin Resource Sharing (CORS) AWS customers who use Amazon S3 to host static web pages or store objects used by other web pages can load content securely by configuring an Amazon S3 bucket to explicitly enable cross origin requests Modern browsers use the Same Origin policy to block JavaScript or HTML5 from allowing requests to load content from another site or domain as a way to help ensure that malicious content is not loaded from a less reputable source (such as during cross site scripting attacks) With the Cross Origin Resource S haring (CORS) policy enabled assets such as web fonts and images stored in an Amazon S3 bucket can be safely referenced by external web pages style sheets and HTML5 applications Amazon S3 Glacier Security Like Amazon S3 the Amazon S3 Glacier service p rovides low cost secure and durable storage But where Amazon S3 is designed for rapid retrieval Amazon S3 Glacier is meant to be used as an archival service for data that is not accessed often and for which retrieval times of several hours are suitable  ArchivedAmazon Web Services Amazon Web Services: Overview of Security Processes Page 47 Amazon S3 Glacier stores files as archives within vaults Archives can be any data such as a photo video or document and can contain one or several files You can store an unlimited number of archives in a single vault and can create up to 1000 vault s per region Each archive can contain up to 40 TB of data Data Upload To transfer data into Amazon S3 Glacier vaults you can upload an archive in a single upload operation or a multipart operation In a single upload operation you can upload archives u p to 4 GB in size However customers can achieve better results using the Multipart Upload API to upload archives greater than 100 MB Using the Multipart Upload API allows you to upload large archives up to about 40000 GB The Multipart Upload API call is designed to improve the upload experience for larger archives; it enables the parts to be uploaded independently in any order and in parallel If a multipart upload fails you only need to upload the failed part again and not the entire archive When you upload data to Amazon S3 Glacier you must compute and supply a tree hash Amazon S3 Glacier checks the hash\"),\n",
       "  0.18039576832018223),\n",
       " (Document(metadata={'Title': 'Building_FaultTolerant_Applications_on_AWS', 'Category': 'General'}, page_content=\"durable secure fault  tolerant data storage AWS is responsible for maintaining availability and fault tolerance; you simply pay for the storage that you use Data is stored as objects within resources called buckets and a single object can be up to 5 terabytes in size Behind the scenes Amazon S3 stores objects redundantly on multiple devices across multiple facilities in an AWS Region —so even in the rare case of a failure in an AWS data center you will still have access to your data Amazon S3 is designed for 99999999999% (11 9's) of durability and stores data for millions of applications for companies globally  Amazon S3 is ideal for any kind of object data storage requirements that your application might have Amazon S3 can be accessed using the AWS Management Console by a U RL through a Command Line Interface (CLI) or via API using an SDK with your programming language of choice The versioning feature in Amazon S3 allows you to retain prior versions of objects stored in S3 and also protects against accidental deletions initiated by a misbehaving application Versioning can be enabled for any of your S3 buckets You can also use either S3 Cross Region Repli cation (CRR) to replicate objects in another region or Same Region Replication (SRR) to replicate objects in the same AWS Region for reduced latency security disaster recovery and other use cases In addition to providing highly available storage Amazon S3 provides multiple storage classes to help reduce storage costs while still providing high availability and durability Using S3 Lifecycle policies objects can be transferred to lower cost storage If you are unsure of your data access patterns you can select S3 Intelligent Tiering which automatically move s your data based on changing access patterns ArchivedAmazon Web Services Fault Tolerant Components on AWS Page 12 By using Amazon S3 you can delegate the responsibility of one critical aspect of fault  tolerance —data storage —to AWS  Amazon Elastic File System and Amazon FSx for Windows File Server While Amazon S3 is ideal for applications that can access data as objects many applications store and access data as files Amazon Elastic File System (Amazon EFS) and Amazon FSx for Windows File Server (Amazon FSx) are fully managed AWS services that provide file based storage for applications Amazon EFS provides a simple scalable elastic file sys tem for Linux based workloads File systems grow and shrink on demand and can scale to petabytes of capacity Amazon EFS is a regional service storing data within and across multiple Availability Zones for high availability and durability Applications tha t need access to shared storage from multiple EC2 instances can store data reliably and securely on Amazon EFS Amazon FSx provides a fully managed native Microsoft Windows file system so you can move your Windows based applications that require file stora ge to AWS With Amazon FSx you can launch highly durable and available Windows file systems that can be accessed from up to thousands of application instances Amazon FSx is highly available within a single AZ For applications that require additional lev els of availability Amazon FSx supports the use of Distributed File System (DFS) Replication to enable multi AZ deployments Using either Amazon EFS or Amazon FSx you can provide highly available fault  tolerant file storage to your applications running in AWS Amazon Relational Database Service Amazon Relational Database Service guides you in the setup operat ion and scal ing a relational database in the cloud It provides cost efficient and resizable capacity while automating time consuming administrati on tasks such as hardware provisioning database setup patching  and backups It frees you to focus on your applications so you can give them the fast performance high availability security  and compatibility they need Amazon RDS is available on severa l database instance types and is optimized for memory performance  or I/O  You can choose\"),\n",
       "  0.16363468224327016)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore.similarity_search_with_relevance_scores(\"what is the durability of s3?\", k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve information using Amazon Bedrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "You are an AI assistant tasked with answering questions based on provided context. Your goal is to provide accurate and relevant answers using only the information given.\n",
    "\n",
    "Here is the context you should use to answer the question:\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Now, here is the question you need to answer:\n",
    "\n",
    "<question>\n",
    "{query}\n",
    "</question>\n",
    "\n",
    "Instructions:\n",
    "1. Carefully read and analyze the provided context.\n",
    "2. Identify key information in the context that is relevant to the question.\n",
    "3. Formulate an answer to the question using only the information from the given context.\n",
    "4. If the context does not contain enough information to fully answer the question, state this clearly in your response.\n",
    "5. Do not use any external knowledge or information not present in the provided context.\n",
    "6. Keep your answer concise and to the point, while ensuring it fully addresses the question.\n",
    "\n",
    "Format your response as follows:\n",
    "1. Begin with a brief answer to the question.\n",
    "2. Follow with a more detailed explanation, if necessary.\n",
    "3. If you're quoting directly from the context, use quotation marks and indicate the quote's location in the context.\n",
    "\n",
    "Remember, it's important to rely solely on the given context and not to introduce any external information or assumptions in your answer.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='According to the context provided, Amazon S3 is designed for \"99.999999999% (11 nines) durability per object and 99.99% availability over a one year period.\"\\n\\nThis is stated directly in the following quote from the first document:\\n\"Amazon S3 is designed for 99.999999999% (11 nines) durability per object and 99.99% availability over a one year period.\"\\n\\nThe context also provides some additional details on how Amazon S3 achieves this high durability:\\n\"Objects are redundantly stored on multiple devices across multiple facilities in an Amazon S3 Region. Once stored, Amazon S3 helps maintain the durability of the objects by quickly detecting and repairing any lost redundancy. Amazon S3 also regularly verifies the integrity of data stored using checksums. If corruption is detected, it is repaired using redundant data.\"\\n\\nSo in summary, the context clearly states that Amazon S3 has an extremely high durability of 11 nines (99.999999999%) per object through redundant storage across multiple facilities and continuous verification and repair of data integrity.' additional_kwargs={'usage': {'prompt_tokens': 3436, 'completion_tokens': 250, 'total_tokens': 3686}, 'stop_reason': 'end_turn', 'model_id': 'anthropic.claude-3-sonnet-20240229-v1:0'} response_metadata={'usage': {'prompt_tokens': 3436, 'completion_tokens': 250, 'total_tokens': 3686}, 'stop_reason': 'end_turn', 'model_id': 'anthropic.claude-3-sonnet-20240229-v1:0'} id='run-81fbb70e-bce5-4ef6-ac08-588a297cf845-0' usage_metadata={'input_tokens': 3436, 'output_tokens': 250, 'total_tokens': 3686}\n"
     ]
    }
   ],
   "source": [
    "query = \"what is the durability of s3?\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "\n",
    "def parse_docs(docs):\n",
    "    return {\n",
    "        'query': query,\n",
    "        'context': docs\n",
    "    }\n",
    "\n",
    "\n",
    "llm = ChatBedrock(\n",
    "    model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "    region_name=\"us-east-1\",\n",
    ")\n",
    "\n",
    "chain = vectorstore.as_retriever() | parse_docs | prompt | llm\n",
    "\n",
    "print(chain.invoke(query))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn more: \n",
    "- [Leverage pgvector and Amazon Aurora PostgreSQL for Natural Language Processing, Chatbots and Sentiment Analysis](https://aws.amazon.com/es/blogs/database/leverage-pgvector-and-amazon-aurora-postgresql-for-natural-language-processing-chatbots-and-sentiment-analysis/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete vectorDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore.drop_tables()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
